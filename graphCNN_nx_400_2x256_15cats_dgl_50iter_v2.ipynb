{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graphCNN-nx-400-2x256-15cats-dgl-50iter-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justcherie/QD_Img_capt_plus_KG/blob/master/graphCNN_nx_400_2x256_15cats_dgl_50iter_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNsH1y6FZTdu",
        "colab_type": "code",
        "outputId": "41c6049d-2154-4483-b44b-9b76b4dedfce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STdgEec0ZgII",
        "colab_type": "code",
        "outputId": "166309e9-b20b-4a15-f637-cea2e857a50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pwd\n",
        "%cd '/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord'\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord\n",
            "/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord\n",
            "/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord\n",
            "archive\t\t\t\t    dgl_nx_25_combined_15cats_train.bin\n",
            "archive-codebug\t\t\t    dgl_nx_400_3cats_combined_test.bin\n",
            "dgl_combined_100_test.bin\t    dgl_nx_400_3cats_combined_train.bin\n",
            "dgl_combined_100_train.bin\t    dgl_nx_400_combined_15cats_test.bin\n",
            "dgl_nx_25_combined_15cats_test.bin  dgl_nx_400_combined_15cats_train.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Yt-AKvZk0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  label_dict = { 'alarm': 78 , 'clock': 17 , 'ambulance': 18 , 'angel': 20 , 'ant':27 , 'barn': 30 , 'basket': 33 , \n",
        "'bee': 79 , 'bicycle': 44 , 'book':  51, 'bridge': 55 , 'bulldozer': 63 , 'bus': 64 , 'butterfly': 75 ,\n",
        " 'cactus': 80 , 'castle': 16 , 'cat': 2 , 'chair': 3 , 'couch': 19 , 'crab':  4, 'cruise': 21,\n",
        " 'ship': 22 , 'dolphin': 23 , 'duck': 24 , 'elephant': 25 , 'eye': 26 , 'face': 5 , 'fan': 28 , 'fire hydrant': 29 ,\n",
        " 'firetruck': 6 , 'flamingo': 31 , 'flower':  32, 'garden':  7, 'hand': 34 , 'hedgehog':  35, 'helicopter': 36 , 'kangaroo': 37 , 'key': 38 ,\n",
        " 'lighthouse': 39 , 'lion': 40 , 'map': 41 , 'mermaid': 42 , 'octopus': 43 , 'owl':  9, 'paintbrush':  45, 'palm tree': 46 , 'parrot': 47,\n",
        " 'passport': 48 , 'peas':  49, 'penguin': 50 , 'pig':  10, 'pineapple': 52 , 'postcard': 53 , 'power outlet': 54 , 'rabbit':  11, 'radio': 56 ,\n",
        " 'rain': 57 , 'rhinoceros':  58, 'roller coaster': 59 , 'sandwich': 60 , 'scorpion': 61, 'sea turtle':  62, 'sheep':  12, 'skull':  13, 'snail':  65,\n",
        " 'snowflake':  66, 'speedboat': 67 , 'spider':  68, 'strawberry': 69 , 'swan': 70 , 'swing set': 71 , 'tennis racquet':  72, 'the mona lisa': 73 ,\n",
        " 'toothbrush': 74 , 'truck': 14 , 'whale':  76, 'windmill': 77, 'airplane': 1, 'mosquito':8, 'yoga':15 }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dlPVgSgEec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_path = \"/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord\"\n",
        "file_name_train = \"dgl_nx_400_combined_15cats_train.bin\"\n",
        "file_name_test = \"dgl_nx_400_combined_15cats_test.bin\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLzGfczZtNW",
        "colab_type": "code",
        "outputId": "ec0b9a91-cc4c-4f73-f762-acf18aabbe96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# reference: https://docs.dgl.ai/tutorials/basics/4_batch.html\n",
        "\n",
        "!pip install dgl\n",
        "\"\"\"\n",
        ".. currentmodule:: dgl\n",
        "\n",
        "Batched Graph Classification with DGL\n",
        "=====================================\n",
        "\n",
        "**Author**: `Mufei Li <https://github.com/mufeili>`_,\n",
        "`Minjie Wang <https://jermainewang.github.io/>`_,\n",
        "`Zheng Zhang <https://shanghai.nyu.edu/academics/faculty/directory/zheng-zhang>`_.\n",
        "\n",
        "Graph classification is an important problem\n",
        "with applications across many fields -- bioinformatics, chemoinformatics, social\n",
        "network analysis, urban computing and cyber-security. Applying graph neural\n",
        "networks to this problem has been a popular approach recently (\n",
        "`Ying et al., 2018 <https://arxiv.org/abs/1806.08804>`_,\n",
        "`Cangea et al., 2018 <https://arxiv.org/abs/1811.01287>`_,\n",
        "`Knyazev et al., 2018 <https://arxiv.org/abs/1811.09595>`_,\n",
        "`Bianchi et al., 2019 <https://arxiv.org/abs/1901.01343>`_,\n",
        "`Liao et al., 2019 <https://arxiv.org/abs/1901.01484>`_,\n",
        "`Gao et al., 2019 <https://openreview.net/forum?id=HJePRoAct7>`_).\n",
        "\n",
        "This tutorial demonstrates:\n",
        " * batching multiple graphs of variable size and shape with DGL\n",
        " * training a graph neural network for a simple graph classification task\n",
        "\"\"\"\n",
        "\n",
        "###############################################################################\n",
        "# Simple Graph Classification Task\n",
        "# --------------------------------\n",
        "# In this tutorial, we will learn how to perform batched graph classification\n",
        "# with dgl via a toy example of classifying 8 types of regular graphs as below:\n",
        "#\n",
        "# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/dataset_overview.png\n",
        "#     :align: center\n",
        "#\n",
        "# We implement a synthetic dataset :class:`data.MiniGCDataset` in DGL. The dataset has 8\n",
        "# different types of graphs and each class has the same number of graph samples.\n",
        "\n",
        "from dgl.data import MiniGCDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/ba/d15ce7fb56958f21d4e9815f96344d92c4982f3fb933a0e987e78cb787e5/dgl-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.17.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.1)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z20A7FVLZ0Mf",
        "colab_type": "code",
        "outputId": "e50a6fd6-1ace-4f43-d7c7-3fa616a56e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "from dgl.data.utils import load_graphs\n",
        "#output_path = \"/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord/\"\n",
        "#file_name = \"dgl_combined_100_train.bin\"\n",
        "glist, glabel_dict = load_graphs(os.path.join(output_path, file_name_train))\n",
        "\n",
        "i=2\n",
        "temp=tuple((glist[i],glabel_dict['label'].tolist()[i]))\n",
        "#graph, label = dataset[0]\n",
        "graph, label = temp\n",
        "label_txt = list(label_dict.keys())[list(label_dict.values()).index(label)]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "nx.draw(graph.to_networkx(), with_labels=True,ax=ax)\n",
        "ax.set_title('Class: '.format(label_txt))\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "nx.draw_kamada_kawai(graph.to_networkx(), with_labels=True,ax=ax)\n",
        "ax.set_title('Class: '.format(label_txt))\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom dgl.data.utils import load_graphs\\n#output_path = \"/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord/\"\\n#file_name = \"dgl_combined_100_train.bin\"\\nglist, glabel_dict = load_graphs(os.path.join(output_path, file_name_train))\\n\\ni=2\\ntemp=tuple((glist[i],glabel_dict[\\'label\\'].tolist()[i]))\\n#graph, label = dataset[0]\\ngraph, label = temp\\nlabel_txt = list(label_dict.keys())[list(label_dict.values()).index(label)]\\n\\nfig, ax = plt.subplots()\\nnx.draw(graph.to_networkx(), with_labels=True,ax=ax)\\nax.set_title(\\'Class: \\'.format(label_txt))\\nplt.show()\\n\\nfig, ax = plt.subplots()\\nnx.draw_kamada_kawai(graph.to_networkx(), with_labels=True,ax=ax)\\nax.set_title(\\'Class: \\'.format(label_txt))\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADNRES0kZ6sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Form a graph mini-batch\n",
        "# -----------------------\n",
        "# To train neural networks more efficiently, a common practice is to **batch**\n",
        "# multiple samples together to form a mini-batch. Batching fixed-shaped tensor\n",
        "# inputs is quite easy (for example, batching two images of size :math:`28\\times 28`\n",
        "# gives a tensor of shape :math:`2\\times 28\\times 28`). By contrast, batching graph inputs\n",
        "# has two challenges:\n",
        "#\n",
        "# * Graphs are sparse.\n",
        "# * Graphs can have various length (e.g. number of nodes and edges).\n",
        "#\n",
        "# To address this, DGL provides a :func:`dgl.batch` API. It leverages the trick that\n",
        "# a batch of graphs can be viewed as a large graph that have many disjoint\n",
        "# connected components. Below is a visualization that gives the general idea:\n",
        "#\n",
        "# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png\n",
        "#     :width: 400pt\n",
        "#     :align: center\n",
        "#\n",
        "# We define the following ``collate`` function to form a mini-batch from a given\n",
        "# list of graph and label pairs.\n",
        "\n",
        "import dgl\n",
        "\n",
        "def collate(samples):\n",
        "    # The input `samples` is a list of pairs\n",
        "    #  (graph, label).\n",
        "    graphs, labels = map(list, zip(*samples))\n",
        "    batched_graph = dgl.batch(graphs)\n",
        "    return batched_graph, torch.tensor(labels)\n",
        "\n",
        "###############################################################################\n",
        "# The return type of :func:`dgl.batch` is still a graph (similar to the fact that\n",
        "# a batch of tensors is still a tensor). This means that any code that works\n",
        "# for one graph immediately works for a batch of graphs. More importantly,\n",
        "# since DGL processes messages on all nodes and edges in parallel, this greatly\n",
        "# improves efficiency.\n",
        "#\n",
        "# Graph Classifier\n",
        "# ----------------\n",
        "# The graph classification can be proceeded as follows:\n",
        "#\n",
        "# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/graph_classifier.png\n",
        "#\n",
        "# From a batch of graphs, we first perform message passing/graph convolution\n",
        "# for nodes to \"communicate\" with others. After message passing, we compute a\n",
        "# tensor for graph representation from node (and edge) attributes. This step may\n",
        "# be called \"readout/aggregation\" interchangeably. Finally, the graph\n",
        "# representations can be fed into a classifier :math:`g` to predict the graph labels.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khxu6TFzaCMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Graph Convolution\n",
        "# -----------------\n",
        "# Our graph convolution operation is basically the same as that for GCN (checkout our \n",
        "# `tutorial <https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html>`_). The only difference is\n",
        "# that we replace :math:`h_{v}^{(l+1)} = \\text{ReLU}\\left(b^{(l)}+\\sum_{u\\in\\mathcal{N}(v)}h_{u}^{(l)}W^{(l)}\\right)` by\n",
        "# :math:`h_{v}^{(l+1)} = \\text{ReLU}\\left(b^{(l)}+\\frac{1}{|\\mathcal{N}(v)|}\\sum_{u\\in\\mathcal{N}(v)}h_{u}^{(l)}W^{(l)}\\right)`.\n",
        "# The replacement of summation by average is to balance nodes with different\n",
        "# degrees, which gives a better performance for this experiment.\n",
        "#\n",
        "# Note that the self edges added in the dataset initialization allows us to\n",
        "# include the original node feature :math:`h_{v}^{(l)}` when taking the average.\n",
        "\n",
        "import dgl.function as fn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Sends a message of node feature h.\n",
        "msg = fn.copy_src(src='h', out='m')\n",
        "\n",
        "def reduce(nodes):\n",
        "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
        "    overwrite the original node feature.\"\"\"\n",
        "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
        "    return {'h': accum}\n",
        "\n",
        "class NodeApplyModule(nn.Module):\n",
        "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(NodeApplyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, node):\n",
        "        h = self.linear(node.data['h'])\n",
        "        h = self.activation(h)\n",
        "        return {'h' : h}\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(GCN, self).__init__()\n",
        "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        # Initialize the node features with h.\n",
        "        g.ndata['h'] = feature\n",
        "        g.update_all(msg, reduce)\n",
        "        g.apply_nodes(func=self.apply_mod)\n",
        "        return g.ndata.pop('h')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JVvyZokaKJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Readout and Classification\n",
        "# --------------------------\n",
        "# For this demonstration, we consider initial node features to be their degrees.\n",
        "# After two rounds of graph convolution, we perform a graph readout by averaging\n",
        "# over all node features for each graph in the batch\n",
        "#\n",
        "# .. math::\n",
        "#\n",
        "#    h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\n",
        "#\n",
        "# In DGL, :func:`dgl.mean_nodes` handles this task for a batch of\n",
        "# graphs with variable size. We then feed our graph representations into a\n",
        "# classifier with one linear layer to obtain pre-softmax logits.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            GCN(in_dim, hidden_dim, F.relu),\n",
        "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
        "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, g):\n",
        "        # For undirected graphs, in_degree is the same as\n",
        "        # out_degree.\n",
        "        h = g.in_degrees().view(-1, 1).float()\n",
        "        for conv in self.layers:\n",
        "            h = conv(g, h)\n",
        "        g.ndata['h'] = h\n",
        "        hg = dgl.mean_nodes(g, 'h')\n",
        "        return self.classify(hg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfIEAj_KaOpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Setup and Training\n",
        "# ------------------\n",
        "# We create a synthetic dataset of :math:`400` graphs with :math:`10` ~\n",
        "# :math:`20` nodes. :math:`320` graphs constitute a training set and\n",
        "# :math:`80` graphs constitute a test set.\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create training and test sets.\n",
        "#trainset = MiniGCDataset(320, 10, 20)\n",
        "#testset = MiniGCDataset(80, 10, 20)\n",
        "\n",
        "from dgl.data.utils import load_graphs\n",
        "#output_path = \"/content/drive/My Drive/tensorflow-tutorial/quickdraw_tfrecord\"\n",
        "#file_name_train = \"dgl_combined_100_train.bin\"\n",
        "glist_train, label_dict_train = load_graphs(os.path.join(output_path, file_name_train))\n",
        "#i=2\n",
        "#temp=tuple((glist[i],label_dict['label'].tolist()[i]))\n",
        "\n",
        "import numpy as np\n",
        "#trainset = tuple((glist_train,label_dict_train['label'].tolist()))\n",
        "#trainset = np.array(([glist_train, label_dict_train['label'].tolist()])\n",
        "trainset = list( zip( glist_train, label_dict_train['label'].tolist() ))\n",
        "\n",
        "#file_name_test = \"dgl_combined_100_test.bin\"\n",
        "glist_test, label_dict_test = load_graphs(os.path.join(output_path, file_name_test))\n",
        "testset = list( zip( glist_test, label_dict_test['label'].tolist() ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfoAw43LaWjj",
        "colab_type": "code",
        "outputId": "9527f333-481f-4b57-c24c-797b9a8fbfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "\n",
        "# Use PyTorch's DataLoader and the collate function\n",
        "# defined before.\n",
        "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
        "                         collate_fn=collate)\n",
        "\n",
        "# Create model\n",
        "#trainset_num_classes = len(np.unique(trainset[1]))\n",
        "#from operator import itemgetter \n",
        "#trainset_num_classes = len(np.unique(list(map(itemgetter(1), trainset))))\n",
        "trainset_num_classes = (len(label_dict.values())+1)\n",
        "####\n",
        "trainset_num_classes = 16\n",
        "####\n",
        "#model = Classifier(1, 256, trainset.num_classes)\n",
        "model = Classifier(1, 256, trainset_num_classes)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "model.train()\n",
        "\n",
        "epoch_losses = []\n",
        "num_epoches = 50\n",
        "weights = []\n",
        "labels_batch_run = []\n",
        "for epoch in range(num_epoches):\n",
        "    epoch_loss = 0\n",
        "    for iter, (bg, label) in enumerate(data_loader):\n",
        "        prediction = model(bg)\n",
        "        loss = loss_func(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.detach().item()\n",
        "       \n",
        "    epoch_loss /= (iter + 1)\n",
        "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss 2.5103\n",
            "Epoch 1, loss 2.3767\n",
            "Epoch 2, loss 2.3518\n",
            "Epoch 3, loss 2.3392\n",
            "Epoch 4, loss 2.3308\n",
            "Epoch 5, loss 2.3240\n",
            "Epoch 6, loss 2.3229\n",
            "Epoch 7, loss 2.3204\n",
            "Epoch 8, loss 2.3177\n",
            "Epoch 9, loss 2.3172\n",
            "Epoch 10, loss 2.3155\n",
            "Epoch 11, loss 2.3171\n",
            "Epoch 12, loss 2.3154\n",
            "Epoch 13, loss 2.3125\n",
            "Epoch 14, loss 2.3143\n",
            "Epoch 15, loss 2.3129\n",
            "Epoch 16, loss 2.3117\n",
            "Epoch 17, loss 2.3121\n",
            "Epoch 18, loss 2.3114\n",
            "Epoch 19, loss 2.3105\n",
            "Epoch 20, loss 2.3104\n",
            "Epoch 21, loss 2.3092\n",
            "Epoch 22, loss 2.3110\n",
            "Epoch 23, loss 2.3107\n",
            "Epoch 24, loss 2.3100\n",
            "Epoch 25, loss 2.3073\n",
            "Epoch 26, loss 2.3095\n",
            "Epoch 27, loss 2.3081\n",
            "Epoch 28, loss 2.3086\n",
            "Epoch 29, loss 2.3060\n",
            "Epoch 30, loss 2.3071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4t84uRNqCYH",
        "colab_type": "code",
        "outputId": "9ebe90ac-5b54-443a-8ac8-eff8de98d75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "###############################################################################\n",
        "# The learning curve of a run is presented below:\n",
        "\n",
        "plt.title('cross entropy averaged over minibatches')\n",
        "plt.plot(epoch_losses)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3f3d70ca33ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cross entropy averaged over minibatches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul-h3dKkafqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# The trained model is evaluated on the test set created. Note that for deployment\n",
        "# of the tutorial, we restrict our running time and you are likely to get a higher\n",
        "# accuracy (:math:`80` % ~ :math:`90` %) than the ones printed below.\n",
        "\n",
        "model.eval()\n",
        "# Convert a list of tuples to two lists\n",
        "test_X, test_Y = map(list, zip(*testset[0:1000]))\n",
        "label_Y = test_Y\n",
        "test_bg = dgl.batch(test_X)\n",
        "\n",
        "test_Y = torch.tensor(test_Y).float().view(-1, 1)\n",
        "probs_Y = torch.softmax(model(test_bg), 1)\n",
        "sampled_Y = torch.multinomial(probs_Y, 1)\n",
        "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
        "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
        "    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
        "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
        "    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fE7gZJpakgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_tsne(title,labels,word_tsne):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "   \n",
        "    plt.figure( figsize=[10,10])\n",
        "    for i in range(len(word_tsne)):\n",
        "        x = word_tsne[i,0]\n",
        "        y = word_tsne[i,1]\n",
        "#        plt.scatter(x, y, marker='x', color='red')\n",
        "        plt.scatter(x, y, marker='o')\n",
        "#        plt.text(x+0.3, y+0.3, labels[i], fontsize=9)\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "    \n",
        "    return\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "#layer_tsne = TSNE(n_components=2).fit_transform(model(test_bg).tolist())\n",
        "#layer_tsne = TSNE(n_components=2).fit_transform(model(test_bg).tolist())\n",
        "\n",
        "#plot_tsne(\"Y prior softmax\",label_Y, layer_tsne)\n",
        "\n",
        "eval_list = []\n",
        "for graph in test_X[0:100]:\n",
        "    eval_list.append(model.forward(graph).tolist())\n",
        "    \n",
        "layer_tsne = TSNE(n_components=2).fit_transform(eval_list)\n",
        "#plot_tsne(\"Y prior softmax\",label_Y, layer_tsne)\n",
        "\n",
        "label_Y_text = []\n",
        "for i in label_Y[0:100]:\n",
        "    #label_Y_text.append(label_dict[label_Y[i]])\n",
        "    \n",
        "    label_Y_text.append( list(label_dict.keys())[list(label_dict.values()).index(label_Y[i])] )\n",
        "\n",
        "import seaborn as sns\n",
        "#iris = sns.load_dataset('iris')\n",
        "sns.scatterplot(x=layer_tsne[:,0], y=layer_tsne[:,1],\n",
        "              hue=label_Y_text)\n",
        "plt.title('t-SNE Y prior to softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}